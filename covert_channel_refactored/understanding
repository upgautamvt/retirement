Note: Don't refactor that both send.c and receive.c share the same file. 
This splits the generated assembly. For instance, in my assembly, I could not find
some local labels like .L45

Linux kernel hot patching (or live patching) is done in assembly because of multiple reasons.

Patching is typically done in assembly rather than C because it offers precise control over specific instructions and memory addresses, which is essential for tasks like replacing a jump with a no-op or redirecting execution. Unlike C, where even minor changes can lead to unpredictable compiler optimizations or binary layout shifts, assembly ensures that what you write is exactly what gets executed. This is particularly valuable for avoiding full recompilation, maintaining binary compatibility, or applying hot/live patches in systems like the Linux kernel. Additionally, reverse engineering and debugging workflows often operate directly at the assembly level, making it the natural choice for fine-grained binary modifications without interfering with function signatures, ABI stability, or the surrounding code.


Synchronization Workflow:

    Initial Offset: The sender writes its initial timestamp (t[0]) to shared memory.
    The receiver initializes time1 = t[0] + 20000, while the sender sets time3 = t[0] + 20000.
    This establishes a shared starting point offset by 20,000 TSC cycles (likely to allow time for setup and alignment).

    Lockstep Increments: Both processes increment their timers (time3 for sender, time1 for receiver) by 7000 TSC cycles
    per iteration, creating a synchronized "heartbeat" where they alternate between:

        Busy-waiting (while (time2 < time3/time1): Ensures each iteration starts only after the previous
                                                    one’s 7,000-cycle window expires.

        Execution phase: Running their respective loops (sender’s branch-specific loop, receiver’s measurement loop).

Timing Precision:

    TSC (Time Stamp Counter): A low-level CPU register that counts cycles at the processor’s frequency.
    Both processes use rdtsc() to read it directly, avoiding OS scheduling jitter.

    Busy-Wait Alignment: The loops while (time2 < time3/time1) ensure strict synchronization.
    Even if processes start at different times, they "catch up" to the same schedule through repeated TSC checks.

Microarchitectural Contention:

    Shared CPU Resources: Sender and receiver run on sibling hyper-threads sharing the
    same physical core’s retirement unit, execution ports, and caches.

    Sender’s Activity: Each sender branch (i = j % 8) executes a slightly different loop
    (due to compiler-preserved code variations), causing unique contention patterns in shared resources.

    Receiver’s Measurement: The receiver measures t[j] = rdtsc() - time3—the time taken to
     execute its own empty loop. This time varies due to sender-induced contention
     (e.g., if the sender’s loop stalls the retirement unit, the receiver’s loop takes longer).

Secret Leakage Mechanism:

    Periodic Patterns: The sender cycles through 8 branches (i = j % 8). The receiver observes
    timing variations every 8 iterations, correlating t[j] peaks/dips with the sender’s branch sequence.

    Example: If i=0 causes heavy retirement unit contention, the receiver sees higher t[j] values every
    8th iteration, leaking the sender’s branch sequence (and thus secrets if i depends on sensitive data).

Key Technical Enhancements

Why 20000/7000?:

    20000: A calibration offset to account for initial setup latency (e.g., shared memory synchronization).

    7000: A tuned interval to align with CPU frequency, ensuring sender/receiver execution
    phases overlap long enough to induce measurable contention.

Compiler’s Role:
The explicit per-branch loops (instead of a helper function) preserve code diversity to
ensure distinct machine code per branch, maintaining variability in retirement unit usage.

Hyper-Threading Exploit:
This is a cross-core side-channel attack leveraging hyper-threading. Without shared physical
resources (e.g., separate cores), the attack would fail.



Assembly modification

(1) Atomic Memory-Barriers Added (Inserted XCHG)

The modified code inserts XCHG (exchange) instructions around memory operations where the original had simple MOV or similar instructions. For example:
    Original:
        movl   %eax,(%ebx)       ; store EAX into [EBX]

    Modified:
        xchgl  %eax,(%ebx)       ; atomic exchange: XCHG [EBX], EAX

        On x86, XCHG with a memory operand implicitly uses the LOCK prefix, turning it into a full atomic RMW (Read-Modify-Write) and a full memory barrier. In contrast, a plain MOV store does not by itself prevent reordering. The effect of the inserted XCHG is to force any previous stores to complete and become visible before continuing, ensuring strict ordering. In practice this likely synchronizes memory and/or stalls the CPU pipeline, changing timing. As one source notes, “the implicit lock prefix on an xchg with memory makes it a full memory barrier”
        (and on x86 sequential-consistent stores often use XCHG. This suggests the patch’s intent was to enforce memory synchronization or introduce deliberate delays around critical memory accesses.

Replacing movl %eax, (%ebx) with xchgl %eax, (%ebx) in an assembly .s file enforces a full memory barrier. This means that all memory loads and stores before the xchg must complete before any following instructions proceed.

The xchg instruction is a read-modify-write (RMW) operation and, when used with a memory operand, it implicitly includes a LOCK prefix, making it atomic. This makes the operation expensive — it stalls the CPU pipeline, flushes cache lines, and prevents reordering.

Because of this cost, xchg is often used not just for atomic updates but also to synchronize memory visibility or intentionally delay execution for debugging or timing control.
Note: RMV atomic operations (read modify write: cpu reads value from memory into register, modifies (or swaps) or updates there, writes back to memory from register. All as one indivisible step — no other core, thread, or device can observe or interfere in the middle.
Note: xchg with registers only is not atomic because it doesn't need to be because it happens very fast.
But xchg with memory involving is atomic.
The instruction xchgl %eax, (%ebx) atomically swaps the contents of %eax with the value at memory location (%ebx), reading and then writing to memory.

Note: Full memory barriers, all memory stores and loads before the barrier must complete and be visible before any memory stores or loads after the barrier can begin.
Full memory barriers are used to prevent (1) hardware level instruction reordering (ii) compiler level instructions reordering
LOCK prefix ensures exclusive access to the memory bus, which grabs a lock for a moment on a whole memory line, and this is why it is very expensive and synchronizing.
XCHG: Atomic RMW, Implicit LOCK prefix, acts as full memory barrier (that enforces both load and store ordering at hardware and compiler level), and stalls CPU pipeline and forces cache coherence resolution (because old cache flushing must happen before new starts because of full memory barrier)

(2) Loop/Branch Structure Changed (Replaced LOOP with DEC/JNZ)

    The original code used the LOOP instruction to implement a delay or count-controlled loop. In the patch this was replaced with an explicit decrement-and-branch sequence. For example:

    ; Original version (uses LOOP instruction)
        movl   $count, %ecx
    loop_start:
        ...                ; (loop body doing work)
        loop   loop_start

    ; Modified version (uses DEC and JNZ)
        movl   $count, %ecx
    loop_start:
        ...                ; (same loop body)
        decl   %ecx
        jnz    loop_start

    Replacing LOOP with DEC %ecx + JNZ likely improves performance and predictability. On many modern CPUs the single-byte LOOP instruction is microcoded and can be slower or cause pipeline stalls, whereas separate DEC and conditional jump have more straightforward execution. This change may help the CPU’s branch predictor or reduce instruction overhead. Functionally, it preserves the same loop-count behavior (both stop when %ecx becomes zero) but can execute faster per iteration. The intent is likely performance tuning or altering loop timing: for instance, to avoid the unpredictable latency of LOOP and achieve a more consistent countdown

(3) Inserted No-Op or Padding Instructions (Timing/Alignment)

    Several no-op or dummy instructions were added, likely to adjust timing or alignment without changing core logic. For example, the modified code may include extra nop or harmless XCHG with the same register:

    Original:
        ...            ; no filler here

    Modified:
        nop
        xchgl %eax,%eax      ; exchange register with itself (effectively a pause)
        nop

    Such inserted instructions do not affect register state but consume CPU cycles. They are often used to pad out delays or disrupt instruction scheduling, making execution slower or less predictable. Inserting XCHG %eax, %eax, for instance, acts like a no-op but still incurs the implicit lock-barrier effect (even a register-to-register XCHG can be slower than a true NOP). The presence of extra NOP-style instructions suggests the patch’s author wanted to introduce slight delays or obfuscate the instruction sequence. This can lengthen the loop duration (lowering performance) without altering functional outcomes.

(4) Changed Conditional/Loop Logic

    The modification also tweaks branch conditions in a few places. For example, a comparison or test might have been rewritten:

    Original:
        cmpl   %ebx, %eax ; compares two registers
        jne    label ; if not equal then jump to label

    Note: this original and modified are not equivalent here.
    Modified:
        testl  %eax, %eax ;  bitwise AND EAX with itself
        jle    label  ; jump to label if EAX ≤ 0

    In this hypothetical snippet, changing from CMP/JNE to TEST/JLE alters how the zero and sign flags are set. Such a change can alter branching behavior and timing: TEST %eax,%eax is a common idiom to check if a register is zero, and it leaves the flags in a predictable way for signed comparisons. This may have been done to streamline flag setting or change loop termination conditions. (Without the exact code it’s hard to say, but any change from CMP to TEST typically is for efficiency or clarity.) The performance impact is usually minor, but it can subtly affect how often branches are taken and how the CPU’s branch predictor behaves.


Summary of Intent and Impact

Overall, the patch appears to add synchronization and delay without altering high-level functionality. The inserted XCHG instructions serve as full memory barriers, ensuring strict ordering of memory operations (useful for inter-thread sync or avoiding reorder optimization). Replacing LOOP with DEC/JNZ likely improves loop performance and predictability. Additional NOP/dummy instructions introduce padding, deliberately slowing execution or thwarting simple analysis. In combination, these changes make the send routine execute more slowly or with different timing characteristics (e.g. longer busy-wait loops or forced cache flushes), while maintaining the same outward behavior. The overall effect is a slower, more tightly synchronized loop, which could be intended to throttle the send-rate or obscure the timing of events

Sources: The behavior of XCHG as a locked, full-memory-barrier instruction on x86 is documented (e.g. “the implicit lock prefix on an xchg with memory makes it a full memory barrier"). On x86 the only way to implement a sequentially consistent store without a separate fence is via XCHG. These changes (locks/fences, different loop instructions) match typical strategies for altering timing or synchronization in low-level code.

In signed (i.e., 2's complement form) if the MSB is 1 it means the number is negative. E.g., 10000000 = −128 (MSB is 1 → negative)

movb $127, %al     ; AL = 01111111 (max positive 8-bit signed value)
addb $1, %al       ; AL becomes 10000000 = -128 (in signed 8-bit)
Here signed overflow will occur because if this is 8 bit signed then there can be -128, 127 but when we add 1 to 127, it becomes 128, which is overflow.
We will see overflow flag (OF)


upgautam@amd:~/CLionProjects/retirement/covert_channel_assembly_learning$ gcc -g -O0 -fno-inline -save-temps send.c -o send
upgautam@amd:~/CLionProjects/retirement/covert_channel_assembly_learning$ objdump -d -S send.o > send.lst



Assembly Learning
cmpq %rbx, %r12 ; %r12 - %rbx
cmp = compare
q = quad word (4*16) = 64 bits
%rbx = source operand
%r12 = destination operand

It is equivalent of %r12 - %rbx but doesn't store the result. Instead, it sets the CPU flags (ZF, SF, CF etc.) based on the result so that a subsequent conditional jump (like je, jne, jl, jg, etc.) can act accordingly.
SF: Sign Flag (for signed numbers)
ZF: Zero Flag
CF: Carry Flag (not negative numbers i.e., unsigned)
OF: Overflow Flag (signed)

These flags are stored inside FLAGS registers (i.e., EFLAGS for 32-bit and RFLAGS for 64 bit). These signed flag are called bit fields. 

If destination is greater/equal/smaller than source

je:jump if equal (or jz: jump if zero)
jne: jump if not equal (or jnz)
jg: jump if greater (or jnle: jump if not less or equal) – same thing
jge: jump if greater or equal (jnl: jump if not less)
jl: jump if less (jnge)
jle: jump if less or equal (jng)

ja or jnbe — Jump if above (unsigned >)
jae or jnb — Jump if above or equal (unsigned >=)
jb or jnae — Jump if below (unsigned <)
jbe or jna — Jump if below or equal (unsigned <=)


Signed integers: Represent both positive and negative values using two's complement. This is why there different instuctions for signed and different for unsigned. 


What actually did they modify in send.s?
for(j=0;j<100000;j++){
    	i = j % 8;
        	time2 = rdtsc();
        	while(time2 < time3) time2 = rdtsc();
        	// sender's loop, changed in send.s, lines 626 to 787
        	if(i == 0) {
        	for(z = 0; z < 100; z++){
       	 
        	}
    	} else if(i == 1) {
        	for(z = 0; z < 100; z++){
       	 
        	}
    	} else if(i == 2) {
        	for(z = 0; z < 100; z++){
       	 
        	}
    	}else if(i == 3) {
        	for(z = 0; z < 100; z++){
       	 
        	}
    	}else if(i == 4) {
        	for(z = 0; z < 100; z++){
       	 
        	}
    	}else if(i == 5) {
        	for(z = 0; z < 100; z++){
       	 
        	}
    	}else if(i == 6) {
        	for(z = 0; z < 100; z++){
       	 
        	}
    	} else {
        	for(z = 0; z < 100; z++){
       	 
        	}
    	}
    	time3 += 7000;
    	}

It checks if i==0, if true then jumps to for(z = 0; z < 100; z++){}, if not true then jumps to else if(i == 1). Same logic continues. In case of true, there always happens 100 iterations. 


.L39:
	cmpq	%rbx, %r12
	jb	.L40
	cmpl	$0, -164(%rbp); if i==0
	jne	.L41; jump to i==1 if i==0 false
    
    
	movl	$0, %eax; eax = 0
	jmp	.L42
	.align	32
.L42:
	cmpl	$329, %eax
	jle	.L43; jump to loop
	jmp	.L44; exit loop
.L43:; if eax<=329, eax starts from 0
	adcl	$1, -180(%rbp)
	addl	$1, %eax
.L44:
	addq	$7000, %rbx  ;time3 += 7000;
	addl	$1, -184(%rbp); increase j

Actually, the red above is patched (i.e., modified). If it was non-patched that would be, 
	movl	$0, -180(%rbp); for loop logic from here; z=0; -180(%rbp) holds z
	jmp	.L42
.L42:
	cmpl  $99, -180(%rbp) 	; compare i with 99
	jle   .L43            	; if i <= 99, jump back to .L43
	jmp   .L44            	; else jump to .L44; for loop logic ends here


As, you can see above, instead of 100 iterations, it modifed to 330 iterations. Then it does time3 = time3+7000 also increments j. This is done when i==0.

It means when unmodified, there is 180 iterations and time3 = time3+7000 also increments j, but when modified there are 330 iterations and time3 = time3+7000 also increments j.

when i==1, 
The modified jumps to .L46 and iterates 160 times and does time3 = time3+7000 also increments j

 "time3 = time3+7000 also increments j" is .L44

when i==2, it iterates 400 times, doing nothing inside the loop. In each iteration, it executes 10 nops. And finally does .L44 again. 
When i==3, it runs 30 iterations. In each iteration, it swaps a register with stack memory, increments stack memory at offset -180, and increments Z. This is costly operations (so we will have observable side effects)

when i==4, it executes as original code.

when i==4, 5, 6 or else, it behaves original. 

It means, in the patched (i.e., modfied) we have exact same behavior when i==0/4/5/6/7, and we have different behaviors when i=2/3/4. It means there are 4 different timing behaviors. This is expected. 

4 Unique Timing Behaviors:
100 iterations (for i == 0, i == 4, i == 5, i == 6, i > 6).
180 iterations (for i == 1).
400 iterations with 10 NOPs each (for i == 2).
30 iterations (for i == 3).

